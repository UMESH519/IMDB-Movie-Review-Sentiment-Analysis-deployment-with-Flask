{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Umesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Umesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Umesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Umesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(review):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review = ' '.join(review+emoticons)\n",
    "    # add emoticons to the end\n",
    "\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    review_clean_original, train['sentiment'], random_state=0, test_size=.2)\n",
    "\n",
    "\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# Transform the text data to feature\n",
    "# Only fit training data (to mimic real world)\n",
    "\n",
    "vectorizer.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000)\n",
      "(5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_bag.toarray().shape) # 20,000 reviews, 2,000 feartures. just as expected\n",
    "print(test_bag.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 37)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 126)\t1\n",
      "  (0, 142)\t2\n",
      "  (0, 145)\t1\n",
      "  (0, 147)\t1\n",
      "  (0, 162)\t1\n",
      "  (0, 194)\t2\n",
      "  (0, 205)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 315)\t1\n",
      "  (0, 327)\t2\n",
      "  (0, 335)\t1\n",
      "  (0, 368)\t1\n",
      "  (0, 395)\t1\n",
      "  (0, 411)\t1\n",
      "  (0, 436)\t1\n",
      "  (0, 475)\t1\n",
      "  (0, 480)\t1\n",
      "  (0, 485)\t1\n",
      "  :\t:\n",
      "  (19999, 3301)\t1\n",
      "  (19999, 3385)\t1\n",
      "  (19999, 3551)\t1\n",
      "  (19999, 3643)\t1\n",
      "  (19999, 3762)\t1\n",
      "  (19999, 3824)\t2\n",
      "  (19999, 3877)\t2\n",
      "  (19999, 3885)\t1\n",
      "  (19999, 3886)\t2\n",
      "  (19999, 3914)\t2\n",
      "  (19999, 3970)\t2\n",
      "  (19999, 4057)\t1\n",
      "  (19999, 4095)\t1\n",
      "  (19999, 4102)\t1\n",
      "  (19999, 4191)\t1\n",
      "  (19999, 4248)\t1\n",
      "  (19999, 4279)\t1\n",
      "  (19999, 4473)\t1\n",
      "  (19999, 4474)\t1\n",
      "  (19999, 4619)\t1\n",
      "  (19999, 4699)\t1\n",
      "  (19999, 4766)\t1\n",
      "  (19999, 4811)\t1\n",
      "  (19999, 4853)\t1\n",
      "  (19999, 4892)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(train_bag))# sparse matrix representation\n",
    "\n",
    "print(train_bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Initialize a Random Forest classifier with 50 trees\n",
    "# hyperparameter n_estimators always set in instantiation\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<20000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1584853 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the target variable\n",
    "\n",
    "forest = forest.fit(train_bag, y_train) # can take 20 seconds to run\n",
    "train_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions) # 100% training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics.accuracy_score(y_test,valid_predictions) # 83% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2178,  370],\n",
       "       [ 395, 2057]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Confusion matrix\n",
    "# Is the number of False Positives and True negatives approx 50/50?\n",
    "metrics.confusion_matrix(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 395)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,valid_predictions).ravel()\n",
    "fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=vectorizer.transform([review_cleaner(\"This was a bad movie\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.predict(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('Vectorizer',CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000)),(\"Model\",RandomForestClassifier(n_estimators = 100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('Vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=5000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabula...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"A Very Bad Movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2173,  375],\n",
       "       [ 385, 2067]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving model to disk\n",
    "import pickle\n",
    "pickle.dump(pipeline, open('Sentiment_Analysis.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n",
      "Table created successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('database.db')\n",
    "print (\"Opened database successfully\")\n",
    "\n",
    "conn.execute('CREATE TABLE Results (Review TEXT PRIMARY KEY, Sentiment TEXT)')\n",
    "print (\"Table created successfully\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
